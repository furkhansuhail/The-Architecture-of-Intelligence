"""
train.py â€” Full Fine-Tuning of LLaMA 3.2 1B on RTX 3090

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  THIS IS FULL FINE-TUNING â€” NOT LoRA/QLoRA                        â•‘
â•‘                                                                   â•‘
â•‘  Key differences from your QLoRA notebooks:                       â•‘
â•‘  â€¢ No BitsAndBytesConfig (no quantization)                        â•‘
â•‘  â€¢ No LoraConfig / get_peft_model (no adapters)                   â•‘
â•‘  â€¢ No prepare_model_for_kbit_training                             â•‘
â•‘  â€¢ ALL model parameters are trainable                             â•‘
â•‘  â€¢ Output is the ENTIRE model, not just adapter weights           â•‘
â•‘                                                                   â•‘
â•‘  Memory fits on 3090 via:                                         â•‘
â•‘  â€¢ bf16 mixed precision                                           â•‘
â•‘  â€¢ gradient checkpointing                                         â•‘
â•‘  â€¢ batch_size=1 + gradient accumulation                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


Step 1 â€” prepare_data.py (already ran, data saved to disk)
    This already happened before training. It converted raw text into token IDs:

        "Hello, how are you?" â†’ [9906, 11, 1268, 527, 499, 30]

    These are just integer lookups â€” not yet embeddings. The output saved to disk is a dataset of integer sequences.

Step 2 â€” train.py (what happens each training step)
    This is where the actual model forward pass happens, and it goes through several stages:

    Token IDs â†’ Embedding Layer â€” The model's first layer is an embedding table (a big matrix of shape [128256 vocab Ã— 2048 hidden_dim]).
    Each token ID is used as an index to look up its corresponding vector. So token 9906 pulls out row 9906, which is a 2048-dimensional vector.
    This is where numbers become meaningful vectors.

    Embeddings â†’ Transformer Layers (Ã—16 layers) â€”
    Those vectors flow through the transformer stack: self-attention (tokens attend to each other), feed-forward networks (transform each vector), and layer norms.
    This is where the model "reasons" about the relationships between tokens.

    Final Layer â†’ Prediction â€” The output of the last transformer layer goes through a linear head that projects back to vocabulary size [2048 â†’ 128256].
    This produces a probability distribution over all possible next tokens at each position.

    Loss Computation â€” The labels (which are just input_ids shifted by one) tell the model what the correct next token should have been.
    Cross-entropy loss measures how wrong the prediction was.

    Backpropagation â€” The loss gradient flows backward through every layer, computing how much each of the 1.24 billion parameters contributed to the error.
    Then AdamW updates every parameter to reduce that error.


        So the full flow per training step is:
        Token IDs  â†’  Embedding Lookup  â†’  16 Transformer Layers  â†’  Next-Token Prediction
           [ints]        [vectors]             [attention + FFN]          [probabilities]
                                                                               â†“
                                                                            Loss
                                                                               â†“
                                                                      Backprop + Update
                                                                     (all 1.24B params)

The key thing is: tokenization (text â†’ integers) is not the same as embedding (integers â†’ vectors).

COMPLETE DATA FLOW â€” What happens during training:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OFFLINE (prepare_data.py â€” already ran before this script):
  Raw text        â†’  Tokenizer (BPE)    â†’  Token IDs saved to disk
  "Hello, how"    â†’  [9906, 11, 1268]   â†’  stored as integers on disk

PER TRAINING STEP (this script â€” happens inside trainer.train()):

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STEP 1: DataLoader reads token IDs from dataset                 â”‚
  â”‚                                                                 â”‚
  â”‚   input_ids:  [128000, 9906, 11, 1268, 527, 499, 30, 128001]    â”‚
  â”‚   labels:     [128000, 9906, 11, 1268, 527, 499, 30, 128001]    â”‚
  â”‚                                                                 â”‚
  â”‚   These are just integers â€” no meaning yet. The DataCollator    â”‚
  â”‚   pads shorter sequences in the batch to the same length.       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ STEP 2: Embedding Layer (model.model.embed_tokens)              â”‚
  â”‚                                                                 â”‚
  â”‚   Shape: [128256 vocab_size Ã— 2048 hidden_dim] â€” a lookup table â”‚
  â”‚                                                                 â”‚
  â”‚   Token ID 9906 â†’ looks up row 9906 â†’ gets a 2048-dim vector    â”‚
  â”‚   Token ID 11   â†’ looks up row 11   â†’ gets a 2048-dim vector    â”‚
  â”‚                                                                 â”‚
  â”‚   Result: [seq_len Ã— 2048] matrix of embedding vectors          â”‚
  â”‚                                                                 â”‚
  â”‚   THIS is where integers become meaningful vectors.             â”‚
  â”‚   These embedding weights are TRAINABLE â€” they get updated      â”‚
  â”‚   during backprop just like all other parameters.               â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ STEP 3: 16 Transformer Layers (model.model.layers[0..15])       â”‚
  â”‚                                                                 â”‚
  â”‚   Each layer applies IN ORDER:                                  â”‚
  â”‚                                                                 â”‚
  â”‚   a) RMSNorm â€” normalize the vectors (stabilizes training)      â”‚
  â”‚                                                                 â”‚
  â”‚   b) Self-Attention (with Grouped Query Attention / GQA):       â”‚
  â”‚      â€¢ Q, K, V projections: [2048] â†’ [2048] each                â”‚
  â”‚      â€¢ Each token attends to all PREVIOUS tokens (causal mask)  â”‚
  â”‚      â€¢ "What should I pay attention to?"                        â”‚
  â”‚      â€¢ Output projection: [2048] â†’ [2048]                       â”‚
  â”‚                                                                 â”‚
  â”‚   c) RMSNorm â€” normalize again                                  â”‚
  â”‚                                                                 â”‚
  â”‚   d) Feed-Forward Network (MLP):                                â”‚
  â”‚      â€¢ gate_proj:  [2048] â†’ [8192]  (expand)                    â”‚
  â”‚      â€¢ up_proj:    [2048] â†’ [8192]  (expand)                    â”‚
  â”‚      â€¢ SiLU activation + element-wise multiply                  â”‚
  â”‚      â€¢ down_proj:  [8192] â†’ [2048]  (compress back)             â”‚
  â”‚      â€¢ "Process and transform the information"                  â”‚
  â”‚                                                                 â”‚
  â”‚   e) Residual connections add the input back to the output      â”‚
  â”‚      (prevents vanishing gradients in deep networks)            â”‚
  â”‚                                                                 â”‚
  â”‚   After 16 layers: [seq_len Ã— 2048] contextual representations  â”‚
  â”‚   Every token's vector now encodes info from all prior tokens.  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ STEP 4: Final Norm + LM Head (model.lm_head)                    â”‚
  â”‚                                                                 â”‚
  â”‚   RMSNorm: normalize the final hidden states                    â”‚
  â”‚   Linear:  [2048] â†’ [128256]  (project to vocabulary size)      â”‚
  â”‚                                                                 â”‚
  â”‚   Result: logits â€” a score for every token in the vocabulary    â”‚
  â”‚   at every position in the sequence.                            â”‚
  â”‚                                                                 â”‚
  â”‚   Example at position 3 (after "Hello , how"):                  â”‚
  â”‚     "are"  â†’ score 8.2  (high â€” model is confident)             â”‚
  â”‚     "is"   â†’ score 3.1  (possible but less likely)              â”‚
  â”‚     "zebra"â†’ score -5.0 (very unlikely)                         â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ STEP 5: Loss Computation (Cross-Entropy)                        â”‚
  â”‚                                                                 â”‚
  â”‚   Compare predictions vs labels (shifted by 1):                 â”‚
  â”‚                                                                 â”‚
  â”‚   Position:    0        1       2       3        4              â”‚
  â”‚   Input:     <bos>   "Hello"  ","    "how"    "are"             â”‚
  â”‚   Predict:   "Hello"   ","    "how"   "are"   "you"             â”‚
  â”‚   Label:     "Hello"   ","    "how"   "are"   "you"             â”‚
  â”‚                                                                 â”‚
  â”‚   Loss = how wrong were the predictions?                        â”‚
  â”‚   Lower loss = model predicted the training data better.        â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ STEP 6: Backpropagation                                         â”‚
  â”‚                                                                 â”‚
  â”‚   Loss gradient flows BACKWARD through every layer:             â”‚
  â”‚     lm_head â†’ layer 15 â†’ layer 14 â†’ ... â†’ layer 0 â†’ embed       â”‚
  â”‚                                                                 â”‚
  â”‚   Computes: "how much did each parameter contribute to the      â”‚
  â”‚   error?" â€” this is the gradient for each of 1.24B params.      â”‚
  â”‚                                                                 â”‚
  â”‚   Gradient checkpointing (enabled): instead of storing all      â”‚
  â”‚   intermediate activations in VRAM, re-computes them during     â”‚
  â”‚   backprop. Trades ~30% more compute for ~40% less VRAM.        â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ STEP 7: Parameter Update (AdamW optimizer)                      â”‚
  â”‚                                                                 â”‚
  â”‚   For each of the 1,235,814,400 parameters:                     â”‚
  â”‚     â€¢ Update momentum (running average of gradients)            â”‚
  â”‚     â€¢ Update variance (running average of squared gradients)    â”‚
  â”‚     â€¢ Compute adaptive learning rate per parameter              â”‚
  â”‚     â€¢ Apply weight decay (regularization)                       â”‚
  â”‚     â€¢ new_weight = old_weight - lr * adjusted_gradient          â”‚
  â”‚                                                                 â”‚
  â”‚   AdamW states are kept in FP32 for numerical stability,        â”‚
  â”‚   even though the model weights are in BF16.                    â”‚
  â”‚                                                                 â”‚
  â”‚   With gradient_accumulation_steps=8, this update only happens  â”‚
  â”‚   every 8 mini-batches. Gradients accumulate across batches     â”‚
  â”‚   giving an effective batch size of 8 (1 Ã— 8).                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  This cycle repeats for every batch across all 3 epochs:
    46,584 examples Ã· 8 effective batch = 5,823 steps/epoch Ã— 3 = ~17,469 total steps


"""

import os
import yaml
import torch
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from prepare_data import load_and_prepare_dataset


def load_config() -> dict:
    config_path = Path(__file__).parent.parent / "configs" / "training_config.yaml"
    with open(config_path) as f:
        return yaml.safe_load(f)


def main():
    config = load_config()

    print("=" * 60)
    print("  FULL FINE-TUNING (all parameters trainable)")
    print("=" * 60)
    print(f"  Model:  {config['model_name']}")
    print(f"  Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
    print()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. Load Tokenizer
    #
    #    Downloads the vocabulary files (~few MB), NOT the model.
    #    The tokenizer converts text â†’ token IDs (integers).
    #    Llama 3 uses BPE via tiktoken with 128,256 vocab tokens.
    #
    #    We need this BEFORE training to:
    #    a) Tokenize the dataset (done in prepare_data.py)
    #    b) Handle padding during batch creation
    #    c) Save alongside the model so inference uses the same vocab
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ“¦ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(config["model_name"], use_fast=True)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. Load Model â€” NO quantization, full precision
    #
    #    Downloads the full model weights (~2.5 GB in bf16).
    #    This is the actual neural network with 1.24B parameters:
    #
    #    MODEL ARCHITECTURE (what gets loaded):
    #    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    #    â”‚  embed_tokens: [128256 Ã— 2048]  â€” Embedding table  â”‚
    #    â”‚    Token IDs â†’ 2048-dim vectors (lookup, not math) â”‚
    #    â”‚                                                    â”‚
    #    â”‚  layers[0..15]: 16 Transformer blocks, each with:  â”‚
    #    â”‚    â”œâ”€ self_attn: Q,K,V,O projections (attention)   â”‚
    #    â”‚    â”œâ”€ mlp: gate_proj, up_proj, down_proj (FFN)     â”‚
    #    â”‚    â””â”€ input/post_attention layernorms (RMSNorm)    â”‚
    #    â”‚                                                    â”‚
    #    â”‚  norm: final RMSNorm                               â”‚
    #    â”‚  lm_head: [2048 â†’ 128256]  â€” Predicts next token   â”‚
    #    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    #
    #    bf16: each parameter = 2 bytes (vs 4 bytes for fp32)
    #    So 1.24B params Ã— 2 bytes â‰ˆ 2.5 GB for weights alone
    #
    #    THIS IS THE KEY DIFFERENCE FROM QLORA:
    #    - No BitsAndBytesConfig (no 4-bit quantization)
    #    - Model loaded in bf16 (full precision, not compressed)
    #    - All parameters will be trained (no frozen layers)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ“¦ Loading model in bf16 (full weights, no quantization)...")
    model = AutoModelForCausalLM.from_pretrained(
        config["model_name"],
        dtype=torch.bfloat16,        # bf16 for memory savings
        device_map="auto",                  # Place on GPU automatically
        attn_implementation="sdpa",         # Efficient attention (PyTorch 2.0+)
    )



    # Enable gradient checkpointing â€” critical for fitting in 24GB
    # Gradient checkpointing: during backprop, instead of storing ALL
    # intermediate activations from the forward pass (very VRAM-hungry),
    # it discards them and re-computes them on the fly.
    # Cost: ~30% more compute. Savings: ~40% less VRAM.
    # Without this, a 1.24B model won't fit on 24GB with optimizer states.
    if config.get("gradient_checkpointing", True):
        model.gradient_checkpointing_enable()
        print("  âœ“ Gradient checkpointing enabled (saves ~40% VRAM)")


    # Verify: ALL parameters are trainable
    # In full fine-tuning, every single weight gets updated.
    # (In LoRA, only ~0.1-1% of params would be trainable)
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"\n  Total parameters:     {total_params:,}")
    print(f"  Trainable parameters: {trainable_params:,}")
    print(f"  Trainable %:          {100 * trainable_params / total_params:.1f}%")
    assert trainable_params == total_params, "Not all parameters are trainable!"
    print(f"  âœ“ Confirmed: 100% of parameters are trainable (FULL fine-tuning)\n")

    # VRAM BUDGET (approximate for 1.24B params in bf16):
    #   Model weights:      ~2.5 GB  (1.24B Ã— 2 bytes)
    #   Gradients:          ~2.5 GB  (same size as weights)
    #   AdamW optimizer:   ~10.0 GB  (2 Ã— FP32 states = 1.24B Ã— 4 bytes Ã— 2)
    #   Activations:        ~3-5 GB  (reduced by gradient checkpointing)
    #   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #   Total:             ~18-20 GB  (fits in 24GB 3090 with headroom)
    print()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. Load & Prepare Dataset
    #
    #    The dataset was already tokenized by prepare_data.py.
    #    Each example is a dict with:
    #      input_ids: [128000, 9906, 11, ...]  â€” token integers
    #      labels:    [128000, 9906, 11, ...]  â€” same (for next-token prediction)
    #      attention_mask: [1, 1, 1, ...]      â€” which tokens to attend to
    #
    #    NO embeddings yet â€” just integers. The embedding lookup
    #    happens inside the model's forward pass (Step 2 in the
    #    data flow diagram above).
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    train_dataset, eval_dataset = load_and_prepare_dataset(config, tokenizer)

    # DataCollator: handles dynamic padding at batch time.
    #   - Each example has a different length (e.g., 87, 234, 156 tokens)
    #   - The collator pads all examples in a batch to the longest one
    #   - mlm=False means Causal LM (predict next token), not Masked LM (BERT-style)
    #
    #   - It also shifts labels left by 1 so the model predicts the NEXT token:
    #     input:  [<bos>, "Hello", ",", "how", "are"]
    #     label:  ["Hello", ",", "how", "are", "you"]
    # Data collator handles dynamic padding + label shifting
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM, not masked LM
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. Training Arguments
    #
    #    Standard HuggingFace Trainer â€” no SFTTrainer needed
    #    since we're doing vanilla supervised fine-tuning
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    training_args = TrainingArguments(
        output_dir=config["output_dir"],

        # Training
        num_train_epochs=config["num_train_epochs"],
        per_device_train_batch_size=config["per_device_train_batch_size"],
        per_device_eval_batch_size=config["per_device_eval_batch_size"],
        gradient_accumulation_steps=config["gradient_accumulation_steps"],

        # Optimizer
        learning_rate=config["learning_rate"],
        weight_decay=config["weight_decay"],
        warmup_steps=int(config.get("warmup_ratio", 0.03) * (
            len(train_dataset) / config["per_device_train_batch_size"] / config["gradient_accumulation_steps"]
        ) * config["num_train_epochs"]),  # Convert warmup_ratio to steps
        lr_scheduler_type=config["lr_scheduler_type"],
        optim=config.get("optim", "adamw_torch_fused"),

        # Precision
        bf16=config.get("bf16", True),

        # Gradient checkpointing (already enabled on model, but Trainer needs to know)
        gradient_checkpointing=config.get("gradient_checkpointing", True),
        gradient_checkpointing_kwargs={"use_reentrant": False},

        # Logging
        logging_steps=config.get("logging_steps", 10),
        report_to=config.get("report_to", "tensorboard"),

        # Evaluation & Saving
        eval_strategy=config.get("eval_strategy", "steps"),
        eval_steps=config.get("eval_steps", 200),
        save_strategy=config.get("save_strategy", "steps"),
        save_steps=config.get("save_steps", 500),
        save_total_limit=config.get("save_total_limit", 2),
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",

        # Other
        seed=config.get("seed", 42),
        dataloader_num_workers=config.get("dataloader_num_workers", 2),
        remove_unused_columns=False,
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5. Create Trainer & Train
    #
    #    The Trainer orchestrates the entire training loop:
    #
    #    for each epoch (3 total):
    #      for each batch of token IDs:
    #        â‘  DataLoader loads batch of input_ids (integers)
    #        â‘¡ Forward pass:
    #           input_ids â†’ Embedding lookup â†’ 2048-dim vectors
    #           â†’ 16 Transformer layers (attention + FFN)
    #           â†’ LM head â†’ logits (scores for all 128K vocab tokens)
    #        â‘¢ Loss: cross-entropy between predicted vs actual next tokens
    #        â‘£ Backward pass: compute gradients for ALL 1.24B parameters
    #        â‘¤ Every 8 batches (gradient_accumulation_steps):
    #           AdamW updates all parameters using accumulated gradients
    #        â‘¥ Every 200 steps: evaluate on eval set (compute eval_loss)
    #        â‘¦ Every 400 steps: save a checkpoint to disk
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        processing_class=tokenizer,  # renamed from 'tokenizer' in transformers v5+
    )

    print("ğŸš€ Starting full fine-tuning...")
    print(f"   Effective batch size: {config['per_device_train_batch_size']} Ã— {config['gradient_accumulation_steps']} = {config['per_device_train_batch_size'] * config['gradient_accumulation_steps']}")
    print()

    trainer.train()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 6. Save the FULL model (not just adapters!)
    #
    #    After training, every one of the 1.24B parameters has
    #    been modified by backprop. We save the ENTIRE model:
    #
    #    What gets saved (~2.5 GB total):
    #      model.safetensors  â€” all updated weights (embed, layers, lm_head)
    #      config.json        â€” model architecture config
    #      tokenizer files    â€” vocab, merges, special tokens
    #
    #    With QLoRA you'd only save ~100MB of adapter deltas.
    #    Here we save the complete, standalone model that can
    #    be loaded directly for inference without any base model.
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    final_dir = os.path.join(config["output_dir"], "final")
    print(f"\nğŸ’¾ Saving full model to {final_dir}")
    trainer.save_model(final_dir)
    tokenizer.save_pretrained(final_dir)

    print("\nâœ… Full fine-tuning complete!")
    print(f"   Model saved to: {final_dir}")
    print(f"   TensorBoard:    tensorboard --logdir {config['output_dir']}")


if __name__ == "__main__":
    main()


"""

How training is done 
The training loop:  yahma/alpaca-cleaned (52K instruction examples) which has 52K instructions 

    * Dataset: 46,584 training examples (52K minus 10% eval split)
        per_device_train_batch_size: 1 (one example at a time â€” VRAM limit)
        gradient_accumulation_steps: 8
        num_train_epochs: 3


EPOCH 1 (full pass through all 46,584 examples):
â”‚
â”œâ”€ Batch 1:  example #1     â†’ forward pass â†’ compute gradients (accumulate)
â”œâ”€ Batch 2:  example #2     â†’ forward pass â†’ compute gradients (accumulate)
â”œâ”€ Batch 3:  example #3     â†’ forward pass â†’ compute gradients (accumulate)
â”œâ”€ Batch 4:  example #4     â†’ forward pass â†’ compute gradients (accumulate)
â”œâ”€ Batch 5:  example #5     â†’ forward pass â†’ compute gradients (accumulate)
â”œâ”€ Batch 6:  example #6     â†’ forward pass â†’ compute gradients (accumulate)
â”œâ”€ Batch 7:  example #7     â†’ forward pass â†’ compute gradients (accumulate)
â”œâ”€ Batch 8:  example #8     â†’ forward pass â†’ â˜… UPDATE all 1.24B weights (step 1)
â”‚
â”œâ”€ Batch 9:  example #9     â†’ forward pass â†’ compute gradients (accumulate)
â”œâ”€ ...
â”œâ”€ Batch 16: example #16    â†’ forward pass â†’ â˜… UPDATE weights (step 2)
â”‚
â”œâ”€ ... continues ...
â”‚
â”œâ”€ Batch 46,584: last example â†’ â˜… UPDATE weights (step 5,823)
â”‚
EPOCH 2 (same 46,584 examples again, shuffled differently):
â”œâ”€ Batch 1:  example #37201  â†’ forward pass â†’ accumulate...
â”œâ”€ ...
â”œâ”€ Step 5,824 through step 11,646
â”‚
EPOCH 3 (same examples, shuffled again):
â”œâ”€ ...
â”œâ”€ Step 11,647 through step 17,469
â”‚
DONE â€” total of ~17,469 optimizer steps


Each "instruction" in the Alpaca dataset looks like this:

{
  "instruction": "Explain the difference between a list and a tuple in Python.",
  "input": "",
  "output": "A list is mutable, meaning you can change its contents..."
}
```

After `prepare_data.py` processes it, that becomes a single sequence of token IDs â€” let's say 187 tokens long. That's **one example, one instruction, one training sample**.

With your config:

**`per_device_train_batch_size: 1`** means the GPU processes **1 instruction per forward pass**. 

That single instruction goes through embedding â†’ 16 transformer layers â†’ loss â†’ backward pass. 

Then the gradients sit in memory.

**`gradient_accumulation_steps: 8`** means it repeats this for **8 separate instructions** before updating weights.

So one "optimizer step" looks like:
```
Instruction 1: "Explain lists vs tuples..."         â†’ forward â†’ backward â†’ hold gradients
Instruction 2: "Write a poem about rain..."         â†’ forward â†’ backward â†’ add to gradients
Instruction 3: "Translate hello to French..."       â†’ forward â†’ backward â†’ add to gradients
Instruction 4: "List 3 benefits of exercise..."     â†’ forward â†’ backward â†’ add to gradients
Instruction 5: "What is machine learning?..."       â†’ forward â†’ backward â†’ add to gradients
Instruction 6: "Sort this list: [3,1,2]..."         â†’ forward â†’ backward â†’ add to gradients
Instruction 7: "Summarize this paragraph..."        â†’ forward â†’ backward â†’ add to gradients
Instruction 8: "Fix this Python code..."            â†’ forward â†’ backward â†’ â˜… UPDATE weights

1 instruction per batch, 8 instructions per weight update.

The reason it's 1 and not, say, 32 is purely a VRAM constraint. 

With a batch size of 32, the GPU would need to hold 32 sets of activations simultaneously during the forward pass â€” that would blow past 24 GB(3090). 

Gradient accumulation is the workaround: you get the mathematical benefit of a larger batch without the memory cost, 
at the expense of taking 8Ã— more forward passes per update.
"""