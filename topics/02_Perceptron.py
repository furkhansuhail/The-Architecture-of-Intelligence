"""
Perceptron - The simplest neural network unit
============================================

A Single Cell Neuron is the simplest type of artificial neuron and the fundamental building block of neural networks.

"""

import base64
import os

TOPIC_NAME = "Single Cell Neuron: Perceptron"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# IMAGE HELPER â€” converts local images to base64 HTML for st.markdown()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _image_to_html(path, alt="", width="100%"):
    """Convert a local image file to an HTML <img> tag with base64 data.
    This allows images to render inside st.markdown() with unsafe_allow_html=True.
    """
    if os.path.exists(path):
        with open(path, "rb") as f:
            b64 = base64.b64encode(f.read()).decode()
        ext = os.path.splitext(path)[1].lstrip(".").lower()
        mime = {"png": "image/png", "jpg": "image/jpeg", "jpeg": "image/jpeg",
                "gif": "image/gif", "svg": "image/svg+xml"}.get(ext, "image/png")
        return f'<img src="data:{mime};base64,{b64}" alt="{alt}" style="width:{width}; border-radius:8px; margin:12px 0;">'
    return f'<p style="color:red;">âš ï¸ Image not found: {path}</p>'


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# THEORY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

THEORY = """

### What is a Perceptron?

A perceptron is the simplest type of artificial neuron and the fundamental building block of neural networks. 
It is inspired by the way biological neurons process information in the brain. At its core, a perceptron takes 
multiple input signals, processes them through a simple mathematical function, and produces a single output.

Think of it like a tiny decision-maker: it receives evidence (inputs), weighs how important each piece of evidence is (weights), 
and then makes a yes-or-no decision (output).

The Biological Analogy
A biological neuron receives electrical signals through its dendrites, processes them in the cell body (soma), 
and if the combined signal is strong enough, it "fires" an output signal through its axon. The perceptron mimics this:
â€¢	Inputs:   (xâ‚, xâ‚‚, ..., xâ‚™) - analogous to dendrites receiving signals
â€¢	Weights:  (wâ‚, wâ‚‚, ..., wâ‚™) - analogous to the strength of synaptic connections
â€¢	Summation function: - analogous to the cell body aggregating signals
â€¢	Activation function: - analogous to the threshold that determines whether the neuron fires
â€¢	Output: - analogous to the axon transmitting a signal


Things that exist inside the network (learnable parameters):
    - Weights â€” strength of connections between neurons, updated during training
    - Bias â€” constant that shifts the decision boundary, updated during training

Things that exist only at setup time (hyperparameters / configuration):
    - Initialization variance â€” controls the spread of random values assigned to weights before training starts, never stored or updated
    - Learning rate â€” controls how much weights change per update step
    - Number of layers, neurons per layer, activation function choices, etc.


### The Math Behind It
## The perceptron computes its output in two steps:


Step 1 â€” Weighted Sum:
    z = Î£(wáµ¢ Â· xáµ¢) + b

Step 2 â€” Activation:
    y = activation(z)   


** ------------ Detailed Explanation ------------ **

### Step 1 â€” Weighted Sum:

                Z = (w1â‹…x1) + (w2â‹…x2) +â‹¯+ (wnâ‹…xn) + b

Or more compactly: 

                z = W Â· X + b

Where:
- X is the input vector
- W is the weight vector
- b is the bias term (a constant that shifts the decision boundary, similar to the y-intercept in a linear equation)


### ** Understanding the Dot Product Intuitively: **

The operation W Â· X is a dot product, and it has a powerful geometric meaning: 
it measures how aligned two vectors are. When the input vector X points in roughly the same direction as the weight vector W, 
the dot product is large and positive â€” the neuron is more likely to fire. When they are orthogonal (at 90Â° to each other), 
the dot product is zero. When they point in opposite directions, it's negative.

This gives us a beautiful intuition for what a perceptron is really doing: 
it's asking "how similar is this input to the pattern I've learned?" The weight vector W encodes the ideal pattern, 
and the dot product measures how closely the current input matches it.
Consider a simple example with two inputs. If the perceptron has learned weights W = [1, 1], 
it's looking for inputs where both features are high.

An input like X = [1, 1] produces a large dot product (1Ã—1 + 1Ã—1 = 2), while X = [-1, -1] produces a strongly negative one (1Ã—(-1) + 1Ã—(-1) = -2).
The perceptron is essentially a pattern matcher.

    # =======================================================================================# 
    **Diagram 1 â€” Vector Alignment and Dot Product Value:**

    DOT PRODUCT: How "aligned" are W and X?

    W Â· X = |W| Ã— |X| Ã— cos(Î¸)     where Î¸ = angle between them


    CASE 1: Same Direction (Î¸ = 0Â°)          CASE 2: Perpendicular (Î¸ = 90Â°)
    cos(0Â°) = 1 â†’ LARGE POSITIVE             cos(90Â°) = 0 â†’ ZERO

                    W                                       W
                â†—                                        â†‘
              â†—                                          |
            â†—   X                                        |
          â†—   â†—                                          |
        â†—   â†—                                            |
      â†—   â†—                                              +--------â†’ X

                                                    "No alignment at all"
    "Strongly aligned"                               Neuron is indifferent
    Neuron fires! âœ…                                    Output â‰ˆ 0 âš ï¸


    CASE 3: Opposite Direction (Î¸ = 180Â°)     CASE 4: Partially Aligned (Î¸ = 45Â°)
    cos(180Â°) = -1 â†’ LARGE NEGATIVE           cos(45Â°) â‰ˆ 0.71 â†’ MODERATE POSITIVE

        W                                          W
            â†—                                          â†—
          â†—                                          â†—
        â†—                                          â†—
      â†—                                          â†—
    â†—                                          â†—---------------â†’ X
            â†™
              â†™  X                            "Partially aligned"
                â†™                               Neuron might fire
    "Completely opposed"                        depending on bias
    Neuron stays silent âŒ


    **Diagram 2 â€” Concrete Example with W = [1, 1]:**

    PERCEPTRON PATTERN MATCHING
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Weight Vector W = [1, 1]  â†’  "I'm looking for BOTH features to be high"

                        xâ‚‚
                        â†‘
                        |
            X=[-1,1]    |  X=[1,1]
            WÂ·X = 0     |  WÂ·X = 2 âœ… STRONG MATCH
            âš ï¸ Neutral  |  â˜… Fires!
                        |          â†— W = [1,1]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€+â”€â”€â”€â”€â”€â”€â”€â”€â†—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚
                        |      â†—
            X=[-1,-1]   |  X=[1,-1]
            WÂ·X = -2    |  WÂ·X = 0
            âŒ Opposed  |  âš ï¸ Neutral
                        |
                        |
                        |

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Input X        Dot Product WÂ·X      Interpretation
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    [ 1,  1]       1(1)+1(1) =  2       Same direction â†’ FIRE
    [ 1, -1]       1(1)+1(-1)=  0       Perpendicular  â†’ MAYBE
    [-1,  1]       1(-1)+1(1)=  0       Perpendicular  â†’ MAYBE
    [-1, -1]       1(-1)+1(-1)= -2      Opposite       â†’ SILENT
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    **Diagram 3 â€” The Pattern Matching Analogy:**
    THE PERCEPTRON AS A PATTERN MATCHER
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    The weight vector W is the "ideal pattern" the neuron has learned.
    The dot product asks: "How much does this input look like my pattern?"


        Learned Pattern          Input Signal           Match Score
        (Weight Vector W)        (Input Vector X)       (Dot Product)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ wâ‚ = 0.8    â”‚         â”‚ xâ‚ = 0.9    â”‚
    â”‚ wâ‚‚ = 0.6    â”‚    Â·    â”‚ xâ‚‚ = 0.7    â”‚    =    W Â· X
    â”‚ wâ‚ƒ = -0.2   â”‚         â”‚ xâ‚ƒ = 0.1    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    z = (0.8)(0.9) + (0.6)(0.7) + (-0.2)(0.1)
        =   0.72     +   0.42     +  -0.02
        =   1.12  â† High score! Input matches the learned pattern.


    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                          â”‚
    â”‚  HIGH WÂ·X  â”€â”€â†’  "This input matches my pattern!"  â”€â”€â†’ ğŸ”¥ â”‚
    â”‚                                                          â”‚
    â”‚  ZERO WÂ·X  â”€â”€â†’  "This input is irrelevant to me"  â”€â”€â†’ ğŸ˜ â”‚
    â”‚                                                          â”‚
    â”‚  LOW  WÂ·X  â”€â”€â†’  "This is the OPPOSITE of my pattern"â†’ â„ï¸ â”‚
    â”‚                                                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    **Diagram 4 â€” Full Pipeline (Dot Product â†’ Bias â†’ Activation):**

    FULL PERCEPTRON PIPELINE
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    Inputs X          Weights W         Dot Product    + Bias    Activation
    â”Œâ”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚xâ‚=1.0â”œâ”€â”€â”€â”€Ã—â”€â”€â”€â”€â”¤wâ‚=0.7â”‚â”€â”€â”
    â””â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”˜  â”‚
                            â”œâ”€â”€â–º Î£ (sum) â”€â”€â–º  z  â”€â”€â–º  step(z) â”€â”€â–º OUTPUT
    â”Œâ”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”  â”‚      â–²
    â”‚xâ‚‚=0.5â”œâ”€â”€â”€â”€Ã—â”€â”€â”€â”€â”¤wâ‚‚=0.3â”‚â”€â”€â”˜      â”‚
    â””â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”˜      â”Œâ”€â”€â”´â”€â”€â”€â”
                                   â”‚b=-0.5â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”˜

    Step by step:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. MULTIPLY each input by its weight (element-wise)     â”‚
    â”‚    xâ‚Ã—wâ‚ = 1.0 Ã— 0.7 = 0.70                             â”‚
    â”‚    xâ‚‚Ã—wâ‚‚ = 0.5 Ã— 0.3 = 0.15                             â”‚
    â”‚                                                         â”‚
    â”‚ 2. SUM them up (this IS the dot product WÂ·X)            â”‚
    â”‚    WÂ·X = 0.70 + 0.15 = 0.85                             â”‚
    â”‚    #======================================#             â”‚
    â”‚    "How well does this input match my learned pattern?" â”‚
    â”‚                                                         â”‚
    â”‚ 3. ADD the bias                                         â”‚
    â”‚    z = 0.85 + (-0.5) = 0.35                             â”‚
    â”‚    #======================================#             â”‚
    â”‚    "Am I over my firing threshold?"                     â”‚
    â”‚                                                         â”‚
    â”‚ 4. ACTIVATE                                             â”‚
    â”‚    step(0.35) = 1  â† z â‰¥ 0, so neuron FIRES ğŸ”¥          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    # =======================================================================================# 

### Step 2 â€” Activation:

The weighted sum z is then passed through an activation function. In Rosenblatt's original perceptron, this was a simple step function:

                        output = 1 if z â‰¥ 0

                        output = 0 if z < 0

This means: if the combined weighted input exceeds a threshold, the perceptron "fires" (outputs 1); otherwise, it doesn't (outputs 0).

### The Role of the Bias:

The bias b is a crucial but often misunderstood component. 
Without a bias, the decision boundary of the perceptron must always pass through the origin. The bias allows the perceptron to shift its decision boundary, 
giving it much more flexibility. 

Think of it as how easily the neuron is "triggered" â€” a large positive bias means the neuron fires easily, 
while a large negative bias means it requires very strong input to fire.

### Bias as Threshold â€” A Historical Bridge

In Rosenblatt's original 1958 formulation, there was actually no bias term. Instead, there was an explicit threshold Î¸, 
and the neuron fired if W Â· X â‰¥ Î¸. The modern formulation rewrites this as W Â· X + b â‰¥ 0, where b = -Î¸. 
These are mathematically identical: 

                    W Â· X + b â‰¥ 0 is the same as W Â· X â‰¥ -b is the same as W Â· X â‰¥ Î¸

So the bias is simply the negative of the threshold. 
This is worth knowing because you'll encounter both formulations in textbooks and papers. 

The bias formulation is preferred today because it lets us treat the bias as just another learnable parameter, simplifying the math and code.

### What Can a Single Perceptron Do?
A single perceptron is a linear binary classifier. 
It can learn to separate data into two classes as long as the data is linearly separable â€” 
    meaning you can draw a straight line (or hyperplane in higher dimensions) between the two classes.

**A single perceptron can learn simple logical functions like AND and OR:**

### The Geometric Picture â€” Decision Boundaries:

This is easier to see than you might think. 
The equation wâ‚xâ‚ + wâ‚‚xâ‚‚ + b = 0 is literally the equation of a straight line in 2D space. 
Everything the perceptron does comes down to this line:

The weights wâ‚ and wâ‚‚ determine the orientation (slope) of the line. Specifically, 
the weight vector **W = [wâ‚, wâ‚‚]** is perpendicular to the decision boundary line, pointing toward the "positive" region.

The bias b determines where the line sits â€” how far from the origin it is shifted.

Everything on one side of the line (where **wâ‚xâ‚ + wâ‚‚xâ‚‚ + b â‰¥ 0**) gets classified as 1.
Everything on the other side (where **wâ‚xâ‚ + wâ‚‚xâ‚‚ + b < 0**) gets classified as 0.

In higher dimensions, this line becomes a plane (3D) or a hyperplane (4D+), but the principle is identical: the perceptron carves the input space in half with a flat surface.


A Single Perceptron Can Learn AND and OR:

AND Gate (both inputs must be 1):

                                AND  Gate : (Both must be active)    

                                xâ‚	xâ‚‚	->  Output
                                --------------------
                                0	0	    0
                                0	1	    0
                                1	0	    0
                                1	1	    1



    A perceptron with weights wâ‚=1, wâ‚‚=1, and bias b=-1.5 solves this perfectly.

**Concrete Walkthrough â€” Proving the AND Gate Works:**

    Let's verify every input by hand to make the formula tangible:
        Input (0, 0): z = (1Ã—0) + (1Ã—0) + (-1.5) = -1.5 â†’ z < 0 â†’ output = 0 âœ“
        Input (0, 1): z = (1Ã—0) + (1Ã—1) + (-1.5) = -0.5 â†’ z < 0 â†’ output = 0 âœ“
        Input (1, 0): z = (1Ã—1) + (1Ã—0) + (-1.5) = -0.5 â†’ z < 0 â†’ output = 0 âœ“
        Input (1, 1): z = (1Ã—1) + (1Ã—1) + (-1.5) = 0.5 â†’ z â‰¥ 0 â†’ output = 1 âœ“
    All four cases produce the correct AND output. Geometrically, 

the decision boundary is the line xâ‚ + xâ‚‚ - 1.5 = 0, which is the line xâ‚‚ = -xâ‚ + 1.5. If you plot the four input points on a 2D grid, 
this line passes between (1,1) on one side and the other three points on the other side.

                                    OR Gate
                                (At least one input must be 1)

                                xâ‚   xâ‚‚   â†’   Output
                                --------------------
                                0    0        0
                                0    1        1
                                1    0        1
                                1    1        1

    A perceptron with weights wâ‚=1, wâ‚‚=1, and bias b=-0.5 solves this:
        Input (0, 0): z = (1Ã—0) + (1Ã—0) + (-0.5) = -0.5 â†’ output = 0 âœ“
        Input (0, 1): z = (1Ã—0) + (1Ã—1) + (-0.5) = 0.5 â†’ output = 1 âœ“
        Input (1, 0): z = (1Ã—1) + (1Ã—0) + (-0.5) = 0.5 â†’ output = 1 âœ“
        Input (1, 1): z = (1Ã—1) + (1Ã—1) + (-0.5) = 1.5 â†’ output = 1 âœ“

Notice how only the bias changed between AND and OR. 
Both use wâ‚=1, wâ‚‚=1, but AND uses b=-1.5 (harder to fire â€” needs both inputs) while OR uses b=-0.5 (easier to fire â€” needs just one). 
This perfectly illustrates the role of the bias as a "trigger sensitivity" knob.

### Part 2: Learning and Training

**The Perceptron Learning Algorithm**
The perceptron doesn't come pre-loaded with the correct weights â€” it learns them from data. The training algorithm is elegantly simple:

1) Initialize weights and bias to small random values (or zeros).
2) For each training example (x, y_true):

        Compute the output: 

                            Å· = activation(W Â· X + b)

        Calculate the error: 

                            error = y_true - Å·

        Update the weights: 

                            wáµ¢ = wáµ¢ + Î· Â· error Â· xáµ¢

        Update the bias: 
                            b = b + Î· Â· error

3) Repeat until the error is zero or a maximum number of iterations is reached.

Here, Î· (eta) is the learning rate, a small positive number (e.g., 0.01) that controls how much the weights are adjusted at each step. 
Too large and the model overshoots; too small and learning is painfully slow.

**Intuition Behind the Learning Rate**

Think of it this way: imagine you're blindfolded, standing on a hilly landscape, trying to find the lowest valley. 
Each step you take is guided only by the slope of the ground under your feet. The learning rate is your step size.

* **Too large (e.g., Î· = 10): You take massive leaps. **
    You might jump right over the valley and land on the opposite hill, then leap back, oscillating wildly and never settling down.

* **Too small (e.g., Î· = 0.0001): You take tiny, cautious steps. **
    You're definitely heading in the right direction, but it might take thousands of steps to get anywhere useful.

* **Just right (e.g., Î· = 0.01 to 0.1): **
    You make steady progress toward the valley and converge in a reasonable number of steps.

Note: Unlike gradient descent in deeper networks where the learning rate interacts with smooth, continuous gradients, 
the perceptron learning rule makes discrete corrections     
    â€” the error is always -1, 0, or +1. So the learning rate here simply scales the size of each correction.

**The key insight is that the perceptron only updates when it makes a mistake.**
**If the prediction is correct, nothing changes. If it's wrong, the weights are nudged in the direction that would have produced the correct answer.**

### Why the Update Rule Works â€” Assigning Blame
The key insight is that the perceptron only updates when it makes a mistake. 
If the prediction is correct, nothing changes. If it's wrong, the weights are nudged in the direction that would have produced the correct answer.
But the update rule wáµ¢ = wáµ¢ + Î· Â· error Â· xáµ¢ is doing something cleverly specific. 

Let's break down why it works:

1) Case 1 â€” False Negative (predicted 0, should be 1, error = +1):
    The perceptron didn't fire when it should have. 
    The fix: increase the weights for inputs that were active (xáµ¢ > 0), so that next time the perceptron sees a similar input, 
    the weighted sum will be larger and more likely to cross the threshold. Inactive inputs (xáµ¢ = 0) contributed nothing to the mistake, 
    so their weights don't change.

2) Case 2 â€” False Positive (predicted 1, should be 0, error = -1):
    The perceptron fired when it shouldn't have. 
    The fix: decrease the weights for inputs that were active, so that next time, the weighted sum will be smaller. 
    Again, inputs that were zero had no role in the mistake and are left alone.

3) Case 3 â€” Correct prediction (error = 0):
    Nothing changes. Don't fix what isn't broken.
    This is an elegant form of credit assignment â€” blame for each mistake is distributed proportionally to how much each input contributed to that mistake. 
    An input that was strongly active (large xáµ¢) gets a larger weight adjustment. An input that was zero gets no adjustment at all.


### Concrete Training Walkthrough
Let's train a perceptron to learn the AND function from scratch. We'll start with all zeros and use a learning rate of Î· = 0.1.

    Initial state: wâ‚ = 0, wâ‚‚ = 0, b = 0
    --- Epoch 1 (first pass through all training examples) ---

    Example 1: 

        x = (0, 0), 
        y_true = 0

        z = (0Ã—0) + (0Ã—0) + 0 = 0 â†’ Å· = step(0) = 1 (since z â‰¥ 0)

        error = 0 - 1 = -1

    wâ‚ = 0 + 0.1 Ã— (-1) Ã— 0 = 0

    wâ‚‚ = 0 + 0.1 Ã— (-1) Ã— 0 = 0

    b = 0 + 0.1 Ã— (-1) = -0.1

    State: wâ‚ = 0, wâ‚‚ = 0, b = -0.1


    Example 2: 

        x = (0, 1), 

        y_true = 0

        z = (0Ã—0) + (0Ã—1) + (-0.1) = -0.1 â†’ Å· = step(-0.1) = 0

    error = 0 - 0 = 0 â†’ correct! No update.

    State: wâ‚ = 0, wâ‚‚ = 0, b = -0.1


    Example 3: 
        x = (1, 0), 

        y_true = 0

        z = (0Ã—1) + (0Ã—0) + (-0.1) = -0.1 â†’ Å· = step(-0.1) = 0

    error = 0 - 0 = 0 â†’ correct! No update.

    State: wâ‚ = 0, wâ‚‚ = 0, b = -0.1


    Example 4: 
        x = (1, 1), 

        y_true = 1

        z = (0Ã—1) + (0Ã—1) + (-0.1) = -0.1 â†’ Å· = step(-0.1) = 0

    error = 1 - 0 = 1

    wâ‚ = 0 + 0.1 Ã— 1 Ã— 1 = 0.1

    wâ‚‚ = 0 + 0.1 Ã— 1 Ã— 1 = 0.1

    b = -0.1 + 0.1 Ã— 1 = 0

    State: wâ‚ = 0.1, wâ‚‚ = 0.1, b = 0


**--- End of Epoch 1 ---**

The weights are already starting to take shape â€” both wâ‚ and wâ‚‚ increased because both inputs were active when the perceptron should have fired. 
But the perceptron hasn't converged yet. After several more epochs, the weights will continue to adjust, eventually settling on values that correctly classify
all four AND inputs. 

The exact final values depend on the learning rate and initialization, but the perceptron is guaranteed to find a solution because AND is linearly separable.

Notice how informative even one epoch is: the perceptron made a mistake on (0,0) and on (1,1), and the weight updates from those mistakes pushed it 
in the right direction. The mistakes on the zero-input examples affected only the bias (because the inputs were zero), 
while the mistake on (1,1) affected the weights too. This is the credit assignment principle at work.


### Convergence Theorem

If the data is linearly separable, the perceptron learning algorithm is guaranteed to converge to a solution in a finite number of steps. 
This is a mathematically proven result. 

The proof relies on the fact that each update reduces the angle between the weight vector and the optimal solution vector, so the algorithm must eventually find it.

However, if the data is NOT linearly separable, the perceptron will never converge â€” it will oscillate forever, 
endlessly adjusting weights without finding a perfect boundary. This is another reason why the XOR problem was so devastating: 
researchers saw the perceptron flailing on it and concluded the entire approach was flawed. 
    Note: The Zero Initialization Problem

If every weight in a neural network is initialized to zero, every neuron in a given layer will compute the exact same output, 
receive the exact same gradient during backpropagation, and update in the exact same way. This is called the symmetry problem.
The neurons never differentiate from each other â€” they all learn the same thing forever. You essentially have a network with one effective neuron per layer, 
no matter how wide it is.

For a single perceptron, zero initialization is actually fine (as we showed in our walkthrough above) because there's only one neuron. 
The symmetry problem only arises when you have multiple neurons in the same layer. This is why random initialization is critical for multi-layer networks.

### Part 3: From Perceptron to Deep Learning

**The Multi-Layer Perceptron (MLP)**
    The limitation of a single perceptron (inability to solve non-linear problems like XOR) led to the development of the Multi-Layer Perceptron (MLP), 
    which stacks multiple layers of perceptrons together:

    * Input Layer â€” receives the raw data
    * Hidden Layer(s) â€” intermediate layers that learn increasingly abstract representations
    * Output Layer â€” produces the final prediction

    By combining multiple perceptrons in layers, the network can learn non-linear decision boundaries. 
    For example, XOR can be solved by a network with just one hidden layer containing two neurons.

**How XOR Gets Solved â€” Building Intuition**
    Here's the key insight for why layers solve the XOR problem. Remember, a single perceptron draws one line. 
    Two perceptrons in a hidden layer draw two lines. The output neuron then combines these, effectively saying "fire if you're on the correct side of BOTH lines."

For XOR, imagine two lines cutting the 2D input space:

    * Line 1 (Neuron A): roughly separates "at least one input is on" from "both are off"
    * Line 2 (Neuron B): roughly separates "at least one input is on" from "both are on"

The output neuron then computes something like "Neuron A fires AND Neuron B fires" â€” which captures the XOR region. 
Each hidden neuron does a simple linear split, but by combining them, the network carves out a non-linear region. 
This is the fundamental mechanism of all deep learning: simple linear operations, composed through layers and non-linearities, produce complex non-linear behavior.


**The Critical Shift: From Perceptron Rule to Backpropagation**
The perceptron learning rule (which we walked through above) works perfectly for a single layer â€” but it cannot train multi-layer networks. 

Why? 

Because the step function is not differentiable. At z = 0 it has an infinite slope; everywhere else the slope is zero. 
You can't compute how much to adjust the weights of hidden neurons because you can't propagate error gradients through a step function.
T
his is the real reason modern networks switched to smooth activation functions. It wasn't just about "better" functions â€” 
it was about making the entire network trainable via calculus. 
The backpropagation algorithm (popularized by Rumelhart, Hinton, and Williams in 1986) uses the chain rule of calculus to compute how much each weight in
every layer contributed to the final error, then adjusts them all simultaneously. 
This requires every function in the chain to be differentiable, which the step function is not.


This insight â€” that you need smooth, differentiable activation functions to train deep networks â€” is what truly ended the AI Winter, 
not just the idea of stacking layers.

### Modern Activation Functions

In deep learning, the step function is almost never used. Instead, modern networks use smooth, differentiable activation functions that enable 
gradient-based optimization (backpropagation):

Sigmoid: Ïƒ(z) = 1 / (1 + eâ»á¶») â€” outputs between 0 and 1. 

* You can think of it as a "soft" version of the step function: instead of a hard 0-or-1 jump, it provides a smooth S-shaped curve. 
Historically popular but suffers from vanishing gradients â€” when z is very large or very small, the gradient approaches zero, 
making learning extremely slow in deep networks.

Tanh: tanh(z) â€” outputs between -1 and 1, zero-centered. 
* Mathematically, tanh is just a rescaled sigmoid: 
tanh(z) = 2Ïƒ(2z) - 1. The zero-centering helps with gradient flow in practice.

ReLU: max(0, z) â€” the most widely used today. 
* Stunningly simple: output z if positive, 0 if negative. 
It's computationally fast, doesn't saturate for positive values (no vanishing gradient on that side), and empirically works remarkably well. 
However, it has a "dying ReLU" problem: if a neuron's inputs consistently produce z < 0, the gradient is always zero and the neuron
permanently stops learning.

Leaky ReLU, GELU, Swish â€” modern variants that address ReLU's limitations. 
* Leaky ReLU allows a small negative slope (e.g., 0.01z for z < 0) so neurons never fully die.
* GELU (Gaussian Error Linear Unit) is used in Transformers and provides a smooth approximation of ReLU. 
* Swish (z Ã— Ïƒ(z)) was discovered by neural architecture search and slightly outperforms ReLU in some deep networks.

From Perceptrons to Deep Networks
Every neuron in a modern deep neural network (CNNs, Transformers, etc.) is fundamentally a generalized perceptron: 
it computes a weighted sum of its inputs, adds a bias, and passes the result through a non-linear activation function. 
The magic of deep learning comes from stacking thousands or millions of these simple units in sophisticated architectures and 
training them end-to-end with backpropagation.

The progression is clear:

1) Perceptron (1958): One neuron, step activation, perceptron learning rule. Can only learn linearly separable patterns.

2) Multi-Layer Perceptron (1986): Multiple layers, sigmoid activation, backpropagation. Can learn any continuous function (universal approximation theorem).

3) Modern Deep Learning (2012+): Hundreds of layers, ReLU/GELU activation, sophisticated architectures (convolutions, attention, residual connections), 
trained on massive data with GPU acceleration.


But at their core, all of these are just perceptrons â€” organized, stacked, and trained with increasingly clever techniques. 
Understanding the single perceptron means understanding the atom from which all of deep learning is built.
    """

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# COMPLEXITY / COMPARISON TABLE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

COMPLEXITY = """

"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OPERATIONS â€” Key code snippets for quick reference
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

OPERATIONS = {
    "Perceptron Implementation": {
        "description": "Single Percepton Implementation",
        "runnable": True,
        "code": '''
"""
================================================================================
SINGLE PERCEPTRON IMPLEMENTATION FROM SCRATCH
================================================================================

A perceptron is the simplest artificial neuron. It takes inputs, multiplies each
by a weight, adds a bias, and passes the result through an activation function
to produce an output.

Architecture:

    x1 â”€â”€(w1)â”€â”€â”
               â”‚
    x2 â”€â”€(w2)â”€â”€â”¼â”€â”€â–º [ Î£ weighted sum + bias ] â”€â”€â–º [ activation ] â”€â”€â–º output
               â”‚
    x3 â”€â”€(w3)â”€â”€â”˜

Math:
    z = (x1*w1) + (x2*w2) + ... + (xn*wn) + bias
    output = activation(z)

This script demonstrates:
    1. How a perceptron is structured
    2. How it makes predictions (forward pass)
    3. How it learns from mistakes (training)
    4. Training it to learn AND, OR, and attempting XOR
================================================================================
"""

import random
import math


class Perceptron:
    """
    A single perceptron (artificial neuron).

    Parameters stored inside the perceptron:
        - self.weights : list of floats (one weight per input feature)
        - self.bias    : single float (shifts the decision boundary)

    These are the ONLY learnable parameters. They get updated during training.
    """

    def __init__(self, num_inputs, learning_rate=0.1, activation="step"):
        """
        Initialize the perceptron.

        Args:
            num_inputs (int):
                How many input features the perceptron will receive.
                For a 2-input logic gate (like AND), this would be 2.

            learning_rate (float):
                Controls how much the weights are adjusted after each mistake.
                - Too high (e.g., 10.0): overshoots the correct weights, unstable
                - Too low (e.g., 0.0001): learns very slowly
                - Typical range: 0.01 to 0.1
                This is a HYPERPARAMETER â€” you set it, the model doesn't learn it.

            activation (str):
                Which activation function to use: "step" or "sigmoid"
        """

        # =====================================================================
        # WEIGHT INITIALIZATION
        # =====================================================================
        # We initialize weights to small random values to BREAK SYMMETRY.
        # If all weights were 0, a multi-layer network would never differentiate
        # its neurons. For a single perceptron it doesn't matter as much,
        # but we follow best practice.
        #
        # Here we use simple random initialization between -0.5 and 0.5.
        # In deep learning, you'd use He or Xavier initialization instead.
        # =====================================================================
        self.weights = [random.uniform(-0.5, 0.5) for _ in range(num_inputs)]

        # =====================================================================
        # BIAS INITIALIZATION
        # =====================================================================
        # Bias is typically initialized to 0. It will be learned during training.
        # The bias allows the decision boundary to shift away from the origin.
        # Without it, the perceptron can only learn boundaries passing through
        # the origin, which is very limiting.
        # =====================================================================
        self.bias = 0.0

        # Store hyperparameters (not learned, set by us)
        self.learning_rate = learning_rate
        self.activation_type = activation

    # =========================================================================
    # ACTIVATION FUNCTIONS
    # =========================================================================
    # The activation function decides whether the neuron "fires" or not.
    # It takes the weighted sum (z) and produces the output.
    # =========================================================================

    def activation(self, z):
        """
        Apply the activation function to the weighted sum.

        Args:
            z (float): The weighted sum = (inputs Â· weights) + bias

        Returns:
            float: The perceptron's output
        """
        if self.activation_type == "step":
            # -----------------------------------------------------------------
            # STEP FUNCTION (Original Perceptron - Rosenblatt 1958)
            # -----------------------------------------------------------------
            # Output is binary: 1 if z >= 0, else 0
            # Simple but NOT differentiable (can't use gradient descent)
            # Used with the basic perceptron learning rule instead
            #
            #   Output
            #     1 |          ________
            #       |         |
            #     0 |_________|
            #       +---------+---------
            #                 0        z
            # -----------------------------------------------------------------
            return 1 if z >= 0 else 0

        elif self.activation_type == "sigmoid":
            # -----------------------------------------------------------------
            # SIGMOID FUNCTION
            # -----------------------------------------------------------------
            # Output is continuous between 0 and 1: Ïƒ(z) = 1 / (1 + e^(-z))
            # Smooth and differentiable (needed for gradient descent)
            # Historically important but causes vanishing gradients in deep nets
            #
            #   Output
            #   1.0 |              ___-------
            #       |           /
            #   0.5 |         /
            #       |       /
            #   0.0 |------
            #       +------|------|------
            #             -5     0     5   z
            # -----------------------------------------------------------------
            return 1 / (1 + math.exp(-z))

    # =========================================================================
    # FORWARD PASS (Making a Prediction)
    # =========================================================================

    def predict(self, inputs):
        """
        Compute the perceptron's output for given inputs.

        This is the FORWARD PASS â€” data flows forward through the neuron.

        Steps:
            1. Multiply each input by its corresponding weight
            2. Sum all the weighted inputs
            3. Add the bias
            4. Pass through the activation function

        Args:
            inputs (list): Input values [x1, x2, ..., xn]

        Returns:
            float: The perceptron's output (0 or 1 for step, 0.0-1.0 for sigmoid)
        """

        # Step 1 & 2: Compute the weighted sum
        # z = (x1 * w1) + (x2 * w2) + ... + (xn * wn)
        weighted_sum = 0.0
        for i in range(len(inputs)):
            weighted_sum += inputs[i] * self.weights[i]

        # Step 3: Add the bias
        # z = weighted_sum + bias
        weighted_sum += self.bias

        # Step 4: Apply activation function
        output = self.activation(weighted_sum)

        return output

    # =========================================================================
    # TRAINING (Learning from Mistakes)
    # =========================================================================

    def train(self, training_data, labels, epochs=100):
        """
        Train the perceptron using the Perceptron Learning Rule.

        The perceptron only learns when it makes a MISTAKE. If the prediction
        is correct, weights stay the same. If wrong, weights are nudged in the
        direction that would have given the correct answer.

        Update rules:
            error = true_label - predicted_output

            For each weight:
                w_i = w_i + (learning_rate Ã— error Ã— x_i)

            For bias:
                bias = bias + (learning_rate Ã— error)

        WHY THIS WORKS:
            - If error = 0 (correct prediction): nothing changes
            - If error = 1 (predicted 0, should be 1): weights INCREASE for
              active inputs, making it more likely to fire next time
            - If error = -1 (predicted 1, should be 0): weights DECREASE for
              active inputs, making it less likely to fire next time

        Args:
            training_data (list of lists): Each inner list is one training example
            labels (list): The correct output for each training example
            epochs (int): Number of complete passes through the training data

        Returns:
            list: History of total errors per epoch (for tracking convergence)
        """

        error_history = []

        for epoch in range(epochs):
            total_errors = 0

            # Go through each training example
            for inputs, true_label in zip(training_data, labels):

                # ----- Forward Pass -----
                prediction = self.predict(inputs)

                # ----- Calculate Error -----
                error = true_label - prediction

                # ----- Update Weights and Bias (only if there's an error) -----
                if error != 0:
                    total_errors += 1

                    # Update each weight
                    for i in range(len(self.weights)):
                        # w_i = w_i + (learning_rate Ã— error Ã— x_i)
                        #
                        # Intuition:
                        #   - error is positive (should have fired but didn't)
                        #     â†’ increase weights for inputs that were active (x_i > 0)
                        #   - error is negative (fired but shouldn't have)
                        #     â†’ decrease weights for inputs that were active
                        #   - x_i = 0 â†’ that input had no influence, don't change its weight
                        self.weights[i] += self.learning_rate * error * inputs[i]

                    # Update bias (bias input is always 1, so: bias += lr * error * 1)
                    self.bias += self.learning_rate * error

            error_history.append(total_errors)

            # If no errors in this epoch, we've converged â€” stop early
            if total_errors == 0:
                print(f"  âœ“ Converged at epoch {epoch + 1} with 0 errors!")
                break

        return error_history

    # =========================================================================
    # DISPLAY THE PERCEPTRON'S CURRENT STATE
    # =========================================================================

    def display_state(self):
        """Print the current weights and bias of the perceptron."""
        print(f"  Weights: {[round(w, 4) for w in self.weights]}")
        print(f"  Bias:    {round(self.bias, 4)}")


# =============================================================================
# =============================================================================
#                           DEMONSTRATIONS
# =============================================================================
# =============================================================================


def demonstrate_logic_gate(gate_name, training_data, labels):
    """
    Train a perceptron to learn a logic gate and display results.

    Args:
        gate_name (str): Name of the gate (AND, OR, XOR, etc.)
        training_data (list): Input combinations
        labels (list): Expected outputs
    """
    print(f"\\n{'=' * 60}")
    print(f"  TRAINING PERCEPTRON TO LEARN: {gate_name} GATE")
    print(f"{'=' * 60}")

    # Create a perceptron with 2 inputs (for 2-input logic gates)
    perceptron = Perceptron(num_inputs=2, learning_rate=0.1, activation="step")

    # Show initial random state
    print(f"\\n  --- Before Training (random weights) ---")
    perceptron.display_state()

    print(f"\\n  --- Training... ---")
    error_history = perceptron.train(training_data, labels, epochs=100)

    # Show learned state
    print(f"\\n  --- After Training (learned weights) ---")
    perceptron.display_state()

    # Test on all inputs and show results
    print(f"\\n  --- Results ---")
    print(f"  {'Input':<15} {'Expected':<12} {'Predicted':<12} {'Correct?'}")
    print(f"  {'-' * 50}")

    all_correct = True
    for inputs, expected in zip(training_data, labels):
        predicted = perceptron.predict(inputs)
        correct = "âœ“" if predicted == expected else "âœ—"
        if predicted != expected:
            all_correct = False
        print(f"  {str(inputs):<15} {expected:<12} {predicted:<12} {correct}")

    if all_correct:
        print(f"\\n  âœ… Perceptron successfully learned the {gate_name} gate!")
    else:
        print(f"\\n  âŒ Perceptron FAILED to learn the {gate_name} gate!")
        print(f"     This is expected â€” {gate_name} is NOT linearly separable.")
        print(f"     A single perceptron can only learn linearly separable patterns.")
        print(f"     You would need a Multi-Layer Perceptron (MLP) to solve this.")

    # Show how errors decreased over training
    print(f"\\n  --- Error History (first 10 epochs) ---")
    for i, errors in enumerate(error_history[:10]):
        bar = "â–ˆ" * errors + "â–‘" * (4 - errors)
        print(f"  Epoch {i + 1:>3}: {bar} ({errors} errors)")

    return perceptron


# =============================================================================
# TRAINING DATA FOR LOGIC GATES
# =============================================================================
# Each gate takes two binary inputs and produces one binary output.
#
# Truth Tables:
#
#   AND: Both inputs must be 1         OR: At least one input must be 1
#   [0,0] â†’ 0                          [0,0] â†’ 0
#   [0,1] â†’ 0                          [0,1] â†’ 1
#   [1,0] â†’ 0                          [1,0] â†’ 1
#   [1,1] â†’ 1                          [1,1] â†’ 1
#
#   NAND: NOT AND                       XOR: Exactly one input must be 1
#   [0,0] â†’ 1                          [0,0] â†’ 0
#   [0,1] â†’ 1                          [0,1] â†’ 1
#   [1,0] â†’ 1                          [1,0] â†’ 1
#   [1,1] â†’ 0                          [1,1] â†’ 0  â† This makes it non-linear!
# =============================================================================

# Input combinations (same for all gates)
inputs = [
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1],
]

# Labels for each gate
and_labels = [0, 0, 0, 1]
or_labels = [0, 1, 1, 1]
nand_labels = [1, 1, 1, 0]
xor_labels = [0, 1, 1, 0]  # NOT linearly separable!

# =============================================================================
# RUN THE DEMONSTRATIONS
# =============================================================================

if __name__ == "__main__":

    print("\\n" + "=" * 60)
    print("  PERCEPTRON: THE BUILDING BLOCK OF DEEP LEARNING")
    print("=" * 60)
    print("""
  This script demonstrates a single perceptron learning logic gates.

  Key Concepts:
    â€¢ Weights determine how much each input matters
    â€¢ Bias shifts the decision boundary
    â€¢ The perceptron learns by adjusting weights when it makes errors
    â€¢ It can ONLY learn linearly separable patterns
    """)

    # ---- Learnable Gates (linearly separable) ----
    demonstrate_logic_gate("AND", inputs, and_labels)
    demonstrate_logic_gate("OR", inputs, or_labels)
    demonstrate_logic_gate("NAND", inputs, nand_labels)

    # ---- Non-Learnable Gate (NOT linearly separable) ----
    demonstrate_logic_gate("XOR", inputs, xor_labels)

    # ==========================================================================
    # BONUS: STEP-BY-STEP WALKTHROUGH OF ONE TRAINING ITERATION
    # ==========================================================================
    print(f"\\n{'=' * 60}")
    print(f"  BONUS: STEP-BY-STEP WALKTHROUGH OF ONE PREDICTION + UPDATE")
    print(f"{'=' * 60}")

    print("""
  Let's manually trace what happens when a perceptron processes one example.

  Setup:
    - Input:         [1, 0]
    - True Label:    1  (this should be the OR gate output for [1, 0])
    - Weights:       [0.2, -0.1]  (some initial random values)
    - Bias:          0.0
    - Learning Rate: 0.1
    """)

    # Manual walkthrough
    x = [1, 0]
    true_label = 1
    weights = [0.2, -0.1]
    bias = 0.0
    lr = 0.1

    # Forward pass
    z = (x[0] * weights[0]) + (x[1] * weights[1]) + bias
    prediction = 1 if z >= 0 else 0
    error = true_label - prediction

    print(f"  FORWARD PASS:")
    print(f"    z = (x1 Ã— w1) + (x2 Ã— w2) + bias")
    print(f"    z = ({x[0]} Ã— {weights[0]}) + ({x[1]} Ã— {weights[1]}) + {bias}")
    print(f"    z = {x[0] * weights[0]} + {x[1] * weights[1]} + {bias}")
    print(f"    z = {z}")
    print(f"    activation(z) = step({z}) = {prediction}")
    print(f"    prediction = {prediction}")
    print()
    print(f"  ERROR CALCULATION:")
    print(f"    error = true_label - prediction = {true_label} - {prediction} = {error}")
    print()

    if error == 0:
        print(f"  WEIGHT UPDATE:")
        print(f"    Error is 0 â†’ prediction was correct â†’ NO UPDATE needed!")
    else:
        new_w0 = weights[0] + lr * error * x[0]
        new_w1 = weights[1] + lr * error * x[1]
        new_bias = bias + lr * error

        print(f"  WEIGHT UPDATE:")
        print(f"    w1_new = w1 + (lr Ã— error Ã— x1)")
        print(f"    w1_new = {weights[0]} + ({lr} Ã— {error} Ã— {x[0]})")
        print(f"    w1_new = {new_w0}")
        print()
        print(f"    w2_new = w2 + (lr Ã— error Ã— x2)")
        print(f"    w2_new = {weights[1]} + ({lr} Ã— {error} Ã— {x[1]})")
        print(f"    w2_new = {new_w1}")
        print()
        print(f"    bias_new = bias + (lr Ã— error)")
        print(f"    bias_new = {bias} + ({lr} Ã— {error})")
        print(f"    bias_new = {new_bias}")
        print()
        print(f"  SUMMARY:")
        print(f"    Weights: {weights} â†’ [{new_w0}, {new_w1}]")
        print(f"    Bias:    {bias} â†’ {new_bias}")

    print(f"\\n{'=' * 60}")
    print(f"  END OF DEMONSTRATION")
    print(f"{'=' * 60}")
    print("""
  Key Takeaways:
    1. A perceptron is just: weighted sum â†’ activation â†’ output
    2. It learns by adjusting weights and bias when it makes mistakes  
    3. It can learn AND, OR, NAND (linearly separable)
    4. It CANNOT learn XOR (not linearly separable)
    5. To solve XOR, you need multiple perceptrons â†’ Multi-Layer Perceptron
    6. Every neuron in modern deep learning is a generalized version of this
    """)
'''
    },
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONTENT EXPORT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_content():
    """Return all content for this topic module."""
    from Required_Images.mlp_xor_component import MLP_XOR_HTML, MLP_XOR_HEIGHT

    # Fallback static image (still used if interactive component fails)
    mlp_img = _image_to_html(
        "Required_Images/MultiLayerPreceptron_Breakdown.png",
        alt="Multilayer Perceptron Architecture",
        width="50%"
    )
    theory_with_images = THEORY.replace("{{MLP_IMAGE}}", mlp_img)

    # Interactive component data â€” app.py splits theory_raw at each
    # placeholder and injects an st.components.v1.html() iframe instead.
    interactive_components = [
        {
            "placeholder": "{{MLP_IMAGE}}",
            "html": MLP_XOR_HTML,
            "height": MLP_XOR_HEIGHT,
        }
    ]

    return {
        "theory": theory_with_images,  # fallback (static image)
        "theory_raw": THEORY,  # raw template with placeholders
        "interactive_components": interactive_components,
        "complexity": COMPLEXITY,
        "operations": OPERATIONS,
    }


